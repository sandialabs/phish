Stream Processing Benchmark (SPB)

This set of benchmarks attempts to test the ability of stream
processing algorithms and software to compute on data in real-time, as
it arrives.

We first outline the goals and philosophy that guided the design of
the SPB.  We then define the benchmarks in two parts, a front-end and
back-end.  The front-end generates a stream of data.  We provide code
for this operation which is meant to be used as-is to generate data at
a tunable rate.  The back-end performs a well-defined computation on
the data, and defines a metric for measuring performance.  Reference
implementations of the back-end are provided.  These can be run as-is
on different.  Users may also choose to implement the back-end
themselves via their own streaming algorithms or frameworks, and
measure the performance.  The results section lists performance of
various software solutions on different hardware platforms.

1 Philosophy
2 Stream Generation (front end)
3 Stream Computation (back end)
4 Results

1 Philosophy

Stream processing at high data rates is often subject to various
constraints.  Data must be examined in real-time as it arrives and
then discarded.  Computations must be fast enough to keep up with the
data arrival rate.  Only limited state information about previous data
can be stored to avoid memory overflow.  Processing an infinite stream
requires mechanisms for aging or expiring old data.  In cases where
approximate answers are acceptable, algorithms should be robust enough
to degrade gracefully if newly arriving data is missed or if less
state information is stored.

The goal of the SPB is to provide an objective method for measuring
these trade-offs and the relative performance of various streaming
algorithms and software on current hardware.  To this end, attributes
of the SPB are as follows:

* A performance metric is defined for each benchmark that is a
combination of speed and accuracy.  Thus an implementation can
trade-off the expense of an algorithm versus its robustness.

* The data generation portion of each benchmark is a pre-defined
operation, for which code is provided, and which cannot be modified.
It operates by writing data to one or more UDP socket ports, which the
back-end portion of the benchmark must monitor and read.  The
rationale for this is that we want all implementations of the
benchmark to see a synthetic data stream that mimics real-time arrival
of data which must be captured or lost.

* The data computation portion of each benchmark is defined at a
higher level, i.e. in an algorithmic sense.  The implementation of the
algorithm is left to the user (reference implementations are
provided).  This allows for tailoring of data structures, choice of
low-level algorithms, hardware-specific options, exploitation of
parallelism, use of existing streaming frameworks, and other choices
to be made by users.  The hope is that that there will be a variety of
implementations that allow for creativity on the part of algorithm and
software designers.

* The benchmarks are scalable in the sense they can be run on a
single-core, a multi-core desktop (with shared-memory and accelerators
if desired), and on distributed-memory platforms such as clusters.

2 Stream Generation

Discuss R-MAT generation:
  underlying algorithm and its logarithmic cost
  input parameters (n,a,b,c,d,fraction,seed)
    which to use for different benchmarks
  other command-line input options
  produces pairs of 64-bit ints
    interpret as edges of sparse graph
    interpret as elements of sparse matrix
  can be duplicates, can be inverse edges (I,J) and (J,I)

Discuss bias-powerlaw generation:

command-line switches

-s value = set RNG seed to value (def = internal seed generation based on clock)
-I npackets = generate npackets of messages (def = 10 million)
-b nbundle = each packet contains nbundle datums (def = 50)
-r rate = desired rate of message generation (datums/sec) as integer
          generator will throttle to this rate if necessary via sleep()
          (def = 0 = unthrottled)
-h port = specify output port explicitly (def = 5555)

Do datums need to be serialized (converted to strings)?

Desirable attributes for stream generator:

* want it scalable/parallel so can go from moderate rates to very high rates
* good to have knob to slow it down for debugging
* good to have it deterministic for debugging and finding "correct" answer
    may not be needed when generating in parallel
* may want stream properties to vary over time
* ability to bundle datums into messages at variable granularity

Other data we might generate:

* messages with N random words from dictionary

3 Stream Computation

Ideas for streaming computations:

* find skew edges
* match in- vs out-degree
* detect the not-normal items
* market trending of strings (Twitter like) for advertising
* process transactions (when OK to drop some, e.g. when building model)

Desirable attributes of stream computation:

* answer is a trade-off between speed and accuracy
  i.e. can get more accurate but may then drop datums

* want at least 2 cycles of data lookup

* want something that can be scaled from small to large
    small might run on 1 core
    medium might run on 1 box (multi-core) or shared memory
    large might run in distributed parallel

* good if can compare answers to some gold standard

4 Results

Nothing yet.
